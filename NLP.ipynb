{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBNdQT1lMhkalAiqiqxmRF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mouni-24/DL-Colab/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nMYg18etzVY",
        "outputId": "626a0a78-df85-4005-f19f-0bcaa221da84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFQ18m3qrPGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MtgU80WtzHRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qGoLHDtt_UW",
        "outputId": "0667d537-a453-4046-e9ef-8f41fe61ee1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the 'stopwords' dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load text data\n",
        "text = \"Natural Language Processing is the field of study focused on the interactions between humans and computers using natural language.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Find word frequency distribution\n",
        "fdist = FreqDist(filtered_tokens)\n",
        "\n",
        "# Print the most common words\n",
        "print(fdist.most_common(5))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Fya8hFpYybHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the 'stopwords' dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load text data\n",
        "text = \"Natural Language Processing is the field of study focused on the interactions between humans and computers using natural language.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Find word frequency distribution\n",
        "fdist = FreqDist(filtered_tokens)\n",
        "\n",
        "# Print the most common words\n",
        "print(fdist.most_common(5))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XvI-Ivvybmn",
        "outputId": "1e163e08-e198-4130-a4bd-04bac01d9f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 1), ('Language', 1), ('Processing', 1), ('field', 1), ('study', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEpkJZ1cyoKR",
        "outputId": "1e5fb1fb-2e74-4790-9f71-4a4876a28724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1GgoQyPy9yf",
        "outputId": "c877dbff-32d7-41f3-aa77-4517f1e4b57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4LXSGUuLzhS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import random\n",
        "\n",
        "# Load text data\n",
        "text = \"The quick brown fox jumps over the lazy dog. The lazy dog, peeved to be left out, decides to jump over him.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = text.split()\n",
        "\n",
        "# Create a dictionary of word pairs and their followers\n",
        "word_pairs = {}\n",
        "for i in range(len(tokens) - 2):  # Iterate with enough space for pairs\n",
        "    pair = (tokens[i], tokens[i + 1])  # Define the word pair\n",
        "    follower = tokens[i + 2]  # Get the word that follows the pair\n",
        "    if pair not in word_pairs:\n",
        "        word_pairs[pair] = []\n",
        "    word_pairs[pair].append(follower)\n",
        "\n",
        "# Choose a random starting pair\n",
        "start_pair = random.choice(list(word_pairs.keys()))\n",
        "\n",
        "# Generate a sentence\n",
        "sentence = list(start_pair)\n",
        "for i in range(10):\n",
        "    # Check if the start_pair exists in the word_pairs dictionary\n",
        "    if start_pair in word_pairs:\n",
        "        next_word = random.choice(word_pairs[start_pair])\n",
        "        sentence.append(next_word)\n",
        "        start_pair = (start_pair[1], next_word)\n",
        "    else:\n",
        "        # If the start_pair is not found, break the loop to avoid the KeyError\n",
        "        break\n",
        "\n",
        "# Convert the sentence back to a string\n",
        "generated_text = ' '.join(sentence)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KX8qmxtazwSu",
        "outputId": "a23299d3-a28c-4bf9-d876-1d42a531f204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lazy dog. The lazy dog, peeved to be left out, decides to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the mapping of base words to their morphological variants\n",
        "add_delete_table = {\n",
        "    'bake': ['baker', 'baking', 'bakes'],\n",
        "    'jump': ['jumped', 'jumping', 'jumps'],\n",
        "    'run': ['ran', 'running', 'runs']\n",
        "}\n",
        "\n",
        "def apply_morphology(word):\n",
        "    # Iterate over the base words in the table\n",
        "    for base_word, variants in add_delete_table.items():\n",
        "        # Check if the input word contains the base word\n",
        "        if base_word in word:\n",
        "                print(base_word)\n",
        "\n",
        "# Take input from the user\n",
        "input_word = input(\"Enter a word: \")\n",
        "apply_morphology(input_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS9wBMftxZYH",
        "outputId": "068623ab-991c-43ad-e807-746bcb716dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: runnning\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # This line downloads the necessary data\n",
        "\n",
        "# Define the input text\n",
        "input_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "tokens = nltk.word_tokenize(input_text)\n",
        "\n",
        "# Define the value of n for n-grams\n",
        "n = 3\n",
        "\n",
        "# Generate n-grams from the tokenized text\n",
        "n_grams = ngrams(tokens, n)\n",
        "\n",
        "# Print the generated n-grams\n",
        "for gram in n_grams:\n",
        "    print(gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM22urQ4xZIQ",
        "outputId": "abea24e7-cfa5-43bf-801f-5b15fefc66a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The', 'quick', 'brown')\n",
            "('quick', 'brown', 'fox')\n",
            "('brown', 'fox', 'jumps')\n",
            "('fox', 'jumps', 'over')\n",
            "('jumps', 'over', 'the')\n",
            "('over', 'the', 'lazy')\n",
            "('the', 'lazy', 'dog')\n",
            "('lazy', 'dog', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rT-Daja6yC3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Download the 'brown' corpus and the 'universal_tagset'\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Ensure 'universal_tagset' is downloaded and available\n",
        "# Check if 'universal_tagset' is in the expected location:\n",
        "import os\n",
        "from nltk.data import find\n",
        "\n",
        "tagset_path = find('taggers/universal_tagset/en-brown.map')\n",
        "if not os.path.exists(tagset_path):\n",
        "    # If not found, download it again and specify the download directory:\n",
        "    import nltk\n",
        "    nltk.download('universal_tagset', download_dir=os.path.join(os.path.expanduser(\"~\"), 'nltk_data'))\n",
        "\n",
        "# Download 'punkt_tab' and specify download directory\n",
        "nltk.download('punkt_tab', download_dir=os.path.join(os.path.expanduser(\"~\"), 'nltk_data'))\n",
        "\n",
        "# Train the POS tagger on the Brown corpus\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "brown_sents = brown.sents(categories='news')\n",
        "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
        "bigram_tagger = nltk.BigramTagger(brown_tagged_sents, backoff=unigram_tagger)\n",
        "# Evaluate the performance of the POS tagger on a test set\n",
        "test_tagged_sents = brown.tagged_sents(categories='editorial', tagset='universal')\n",
        "print(bigram_tagger.evaluate(test_tagged_sents))\n",
        "# Use the POS tagger to tag a sample sentence\n",
        "test_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "test_tokens = nltk.word_tokenize(test_sentence)\n",
        "test_tags = bigram_tagger.tag(test_tokens)\n",
        "print(test_tags)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT0XJBJU_LfI",
        "outputId": "80a60b07-cc98-4b6b-90e7-c0264085fe9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "<ipython-input-8-7b0d3121a651>:29: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(bigram_tagger.evaluate(test_tagged_sents))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04863320563599766\n",
            "[('The', 'AT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', None), ('jumps', None), ('over', 'IN'), ('the', 'AT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ]
    }
  ]
}